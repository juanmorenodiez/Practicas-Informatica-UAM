{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Eq_QfGIGXC_"
   },
   "source": [
    "### **Búsqueda y Minería de Información 2021-22**\n",
    "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
    "### Grado en Ingeniería Informática, 4º curso\n",
    "# **Motores de búsqueda e indexación**\n",
    "\n",
    "Fechas:\n",
    "\n",
    "* Comienzo: lunes 21 / martes 22 de febrero\n",
    "* Entrega: lunes 28 / martes 29 de marzo (14:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYT0Qlrnoy7l"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDFY_K6_pA_J"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "Los objetivos de esta práctica son:\n",
    "\n",
    "* La implementación eficiente de funciones de ránking, particularizada en el modelo vectorial.\n",
    "*\tLa implementación de índices eficientes para motores de búsqueda. \n",
    "*\tLa implementación de un método de búsqueda proximal.\n",
    "*\tLa dotación de estructuras de índice posicional que soporten la búsqueda proximal.\n",
    "*\tLa implementación del algoritmo PageRank.\n",
    "\n",
    "Se desarrollarán implementaciones de índices utilizando un diccionario y listas de postings. Y se implementará el modelo vectorial utilizando estas estructuras más eficientes para la ejecución de consultas.\n",
    "\n",
    "Los ejercicios básicos consistirán en la implementación de algoritmos y técnicas estudiados en las clases de teoría, con algunas propuestas de extensión opcionales. Se podrá comparar el rendimiento de las diferentes versiones de índices y buscadores, contrastando la coherencia con los planteamientos estudiados a nivel teórico.\n",
    "\n",
    "Mediante el nivel de abstracción seguido, se conseguirán versiones intercambiables de índices y buscadores. El único buscador que no será intercambiable es el de Whoosh, que sólo funcionará con sus propios índices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPjq_DVVpDEL"
   },
   "source": [
    "## Material proporcionado\n",
    "\n",
    "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
    "\n",
    "*\tVarias clases e interfaces Python a lo largo de este *notebook*, con las que el estudiante integrará las suyas propias. \n",
    "Las clases parten del código de la práctica anterior.\n",
    "Igual que en la práctica 1, la función **main** implementa un programa que deberá funcionar con las clases a implementar por el estudiante.\n",
    "*\tLas colecciones de prueba de la práctica 1: <ins>toys.zip</ins> (que se descomprime en dos carpetas toy1 y toy2), <ins>docs1k.zip</ins> con 1.000 documentos HTML y un pequeño fichero <ins>urls.txt</ins>. \n",
    "*\tUna colección más grande: <ins>docs10k.zip</ins> con 10.000 documentos HTML.\n",
    "*\tVarios grafos para probar PageRank: <ins>graphs.zip</ins>.\n",
    "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la función main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xAKBQZLLqVXR"
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Config(object):\n",
    "  # variables de clase\n",
    "  NORMS_FILE = \"docnorms.dat\"\n",
    "  PATHS_FILE = \"docpaths.dat\"\n",
    "  INDEX_FILE = \"serialindex.dat\"\n",
    "  DICTIONARY_FILE = \"dictionary.dat\"\n",
    "  POSTINGS_FILE = \"postings.dat\"\n",
    "\n",
    "class BasicParser:\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())\n",
    "    \n",
    "def tf(freq):\n",
    "    return 1 + math.log2(freq) if freq > 0 else 0\n",
    "\n",
    "def idf(ndocs_t, ndocs):\n",
    "    return math.log2((ndocs+1)/(ndocs_t+0.5))\n",
    "\n",
    "\"\"\"\n",
    "    This is an abstract class for the search engines\n",
    "\"\"\"\n",
    "class Searcher(ABC):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        self.index = index\n",
    "        self.parser = parser\n",
    "    @abstractmethod\n",
    "    def search(self, query, cutoff):\n",
    "        \"\"\" Returns a list of documents encapsulated in a SearchRanking class \"\"\"\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, dir=None):\n",
    "        self.docmap = []\n",
    "        self.modulemap = {}\n",
    "        if dir: self.open(dir)\n",
    "    def add_doc(self, path):\n",
    "        self.docmap.append(path)  # Assumed to come in order\n",
    "    def doc_path(self, docid):\n",
    "        return self.docmap[docid]\n",
    "    def doc_module(self, docid):\n",
    "        if docid in self.modulemap:\n",
    "            return self.modulemap[docid]\n",
    "        return None\n",
    "    def ndocs(self):\n",
    "        return len(self.docmap)\n",
    "    def doc_freq(self, term):\n",
    "        return len(self.postings(term))\n",
    "    \n",
    "    def term_freq(self, term, docID):\n",
    "        post = self.postings(term)\n",
    "        if post is None: return 0\n",
    "        for posting in post:\n",
    "            if posting[0] == docID:\n",
    "                return posting[1]\n",
    "        return 0\n",
    "    \n",
    "    def total_freq(self, term):\n",
    "        freq = 0\n",
    "        for posting in self.postings(term):\n",
    "            freq += posting[1]\n",
    "        return freq\n",
    "    \n",
    "    def postings(self, term):\n",
    "        return list()\n",
    "    \n",
    "    def positional_postings(self, term):\n",
    "        # used in positional implementations\n",
    "        return list()\n",
    "    \n",
    "    def all_terms(self):\n",
    "        return list()\n",
    "    \n",
    "    def save(self, dir):\n",
    "        if not self.modulemap: self.compute_modules()\n",
    "        p = os.path.join(dir, Config.NORMS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.modulemap, f)        \n",
    "    def open(self, dir):\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.NORMS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.modulemap = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass\n",
    "    def compute_modules(self):\n",
    "        for term in self.all_terms():\n",
    "            idf_score = idf(self.doc_freq(term), self.ndocs())\n",
    "            post = self.postings(term)\n",
    "            if post is None: continue\n",
    "            for docid, freq in post:\n",
    "                if docid not in self.modulemap: self.modulemap[docid] = 0\n",
    "                self.modulemap[docid] += math.pow(tf(freq) * idf_score, 2)\n",
    "        for docid in range(self.ndocs()):\n",
    "            self.modulemap[docid] = math.sqrt(self.modulemap[docid]) if docid in self.modulemap else 0\n",
    "        \n",
    "\n",
    "class Builder:\n",
    "    def __init__(self, dir, parser=BasicParser()):\n",
    "        if os.path.exists(dir): shutil.rmtree(dir)\n",
    "        os.makedirs(dir)\n",
    "        self.parser = parser\n",
    "    def build(self, path):\n",
    "        if zipfile.is_zipfile(path):\n",
    "            self.index_zip(path)\n",
    "        elif os.path.isdir(path):\n",
    "            self.index_dir(path)\n",
    "        else:\n",
    "            self.index_url_file(path)\n",
    "    def index_zip(self, filename):\n",
    "        file = zipfile.ZipFile(filename, mode='r', compression=zipfile.ZIP_DEFLATED)\n",
    "        for name in sorted(file.namelist()):\n",
    "            with file.open(name, \"r\", force_zip64=True) as f:\n",
    "                self.index_document(name, BeautifulSoup(f.read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "        file.close()\n",
    "    def index_dir(self, dir):\n",
    "        for subdir, dirs, files in os.walk(dir):\n",
    "            for file in sorted(files):\n",
    "                path = os.path.join(dir, file)\n",
    "                with open(path, \"r\") as f:\n",
    "                    self.index_document(path, f.read())\n",
    "    def index_url_file(self, file):\n",
    "        with open(file, \"r\") as f:\n",
    "            self.index_urls(line.rstrip('\\n') for line in f)\n",
    "    def index_urls(self, urls):\n",
    "        for url in urls:\n",
    "            self.index_document(url, BeautifulSoup(urlopen(url).read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "    def index_document(self, path, text):\n",
    "        pass\n",
    "    def commit(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "yMoae4N7y38C"
   },
   "outputs": [],
   "source": [
    "# from previous lab\n",
    "class SlowVSMSearcher(Searcher):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        qterms = self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        for docid in range(self.index.ndocs()):\n",
    "            score = self.score(docid, qterms)\n",
    "            if score:\n",
    "                ranking.push(self.index.doc_path(docid), score)\n",
    "        return ranking\n",
    "\n",
    "    def score(self, docid, qterms):\n",
    "        prod = 0\n",
    "        for term in qterms:\n",
    "            prod += tf(self.index.term_freq(term, docid)) \\\n",
    "                    * idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "        mod = self.index.doc_module(docid)\n",
    "        if mod:\n",
    "            return prod / mod\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "I-7gj9Rxx6LD"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import whoosh\n",
    "except ModuleNotFoundError:\n",
    "  !pip install whoosh\n",
    "  import whoosh\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.formats import Format\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# A schema in Whoosh is the set of possible fields in a document in\n",
    "# the search space. We just define a simple 'Document' schema, with\n",
    "# a path (a URL or local pathname) and a content.\n",
    "SimpleDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False))\n",
    "ForwardDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False,vector=Format))\n",
    "PositionalDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=True))\n",
    "\n",
    "class WhooshBuilder(Builder):\n",
    "    def __init__(self, dir, schema=SimpleDocument):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_writer = whoosh.index.create_in(dir, schema).writer(procs=1, limitmb=16384, multisegment=True)\n",
    "        self.dir = dir\n",
    "\n",
    "    def index_document(self, p, text):\n",
    "        self.whoosh_writer.add_document(path=p, content=text)\n",
    "\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshForwardBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, ForwardDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshForwardIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshPositionalBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, PositionalDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshPositionalIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_reader = whoosh.index.open_dir(dir).reader()    \n",
    "    def total_freq(self, term):\n",
    "        return self.whoosh_reader.frequency(\"content\", term)\n",
    "    def doc_freq(self, term):\n",
    "        return self.whoosh_reader.doc_frequency(\"content\", term)\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_reader.stored_fields(docid)['path']\n",
    "    def ndocs(self):\n",
    "        return self.whoosh_reader.doc_count()\n",
    "    def all_terms(self):\n",
    "        return list(self.whoosh_reader.field_terms(\"content\"))\n",
    "    def postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"frequency\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshForwardIndex(WhooshIndex):\n",
    "    def term_freq(self, term, docID) -> int:\n",
    "        if self.whoosh_reader.has_vector(docID, \"content\"):\n",
    "            v = self.whoosh_reader.vector(docID, \"content\")\n",
    "            v.skip_to(term)\n",
    "            if v.id() == term:\n",
    "                return v.value_as(\"frequency\")\n",
    "        return 0\n",
    "\n",
    "class WhooshPositionalIndex(WhooshIndex):\n",
    "    def positional_postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"positions\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshSearcher(Searcher):\n",
    "    def __init__(self, dir):\n",
    "        self.whoosh_index = whoosh.index.open_dir(dir)\n",
    "        self.whoosh_searcher = self.whoosh_index.searcher()\n",
    "        self.qparser = QueryParser(\"content\", schema=self.whoosh_index.schema)\n",
    "    def search(self, query, cutoff):\n",
    "        return map(lambda scoredoc: (self.doc_path(scoredoc[0]), scoredoc[1]),\n",
    "                   self.whoosh_searcher.search(self.qparser.parse(query), limit=cutoff).items())\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_index.reader().stored_fields(docid)['path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKBXprvhpqQr"
   },
   "source": [
    "## Calificación\n",
    "\n",
    "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado. No obstante, aquellos ejercicios marcados con un asterisco (*) tienen una complejidad un poco superior a los demás (que suman 7.5 puntos), y permiten, si se realizan todos, una nota superior a 10. \n",
    "\n",
    "El peso de la nota de esta práctica en la calificación final de prácticas es del **40%**.\n",
    "\n",
    "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. La calidad se valorará por los **resultados** conseguidos (economía de consumo de RAM, disco y tiempo; tamaño de las colecciones que se consigan indexar) pero también del **mérito** en términos del interés de las técnicas aplicadas y la buena programación.\n",
    "\n",
    "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
    "\n",
    "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) integrado con las clases que se facilitan. El profesor comprobará este aspecto añadiendo los módulos entregados por el estudiante a los módulos facilitados en la práctica, ejecutando la función *main* así como otros main de prueba adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfMu2CuGprtk"
   },
   "source": [
    "## Entrega\n",
    "\n",
    "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--pSxv2vpvUg"
   },
   "source": [
    "## Indicaciones\n",
    "\n",
    "Se sugiere trabajar en la práctica de manera incremental, asegurando la implementación de soluciones sencillas y mejorándolas de forma modular (la propia estructura de ejercicios plantea ya esta forma de trabajar).\n",
    "\n",
    "Se podrán definir clases o módulos adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: la función **main** deberá ejecutar correctamente <ins>sin ninguna modificación</ins> (más allá de comentar aquellos ejercicios que no se hayan realizado).\n",
    "\n",
    "Asimismo, se recomienda indexar sin ningún tipo de stopwords ni stemming, para poder hacer pruebas más fácilmente con ejemplos “de juguete”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3jRLNZmpEk_"
   },
   "source": [
    "# Ejercicio 1: Implementación de un modelo vectorial eficiente\n",
    "\n",
    "Se mejorará la implementación de la práctica anterior aplicando algoritmos estudiados en las clases de teoría. En particular, se utilizarán listas de postings en lugar de un índice forward.\n",
    "\n",
    "La reimplementación seguirá haciendo uso de la clase abstracta Index, y se podrá probar con cualquier implementación de esta clase (tanto la implementación de índice sobre Whoosh como las propias). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be3vDQNxdWbo"
   },
   "source": [
    "## Ejercicio 1.1: Método orientado a términos (3pt)\n",
    "\n",
    "Escribir una clase TermBasedVSMSearcher que implemente el modelo vectorial coseno por el método orientado a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ppr9PtZmduql"
   },
   "outputs": [],
   "source": [
    "class TermBasedVSMSearcher(Searcher):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "            \n",
    "    def search(self, query, cutoff):\n",
    "        qterms = self.parser.parse(query)\n",
    "        \n",
    "        scores = {}\n",
    "        for term in qterms:\n",
    "            postings = self.index.postings(term)\n",
    "            for posting in postings:\n",
    "                tf_value = tf(posting[1])\n",
    "                idf_value = idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "                if posting[0] in scores.keys():\n",
    "                    scores[posting[0]] += tf_value*idf_value\n",
    "                else:\n",
    "                    scores[posting[0]] = tf_value*idf_value\n",
    "             \n",
    "        ranking = SearchRanking(cutoff)\n",
    "        for key, value in scores.items():\n",
    "            ranking.push(self.index.doc_path(key), value / self.index.doc_module(key))\n",
    "            \n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3YGEGm7haop"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "El método orientado a términos es una de las implementaciones populares que se pueden dar en el modelo vectorial. Consiste en una iteración secuencial sobre las listas de postings de cada término de la consulta. Como bien se puede ver en la implementación, estas son las dos iteraciones principales que se realizan. Cada posting de la lista contiene el doc_id del documento en el cual se encuentra el término y la frecuencia del término en el documento.\n",
    "\n",
    "Procedemos a calcular el tf-idf de cada término de la consulta en cada documento. Si el doc_id del posting que estamos examinando ya está en el diccionario, simplemente acumulamos el tf-idf de ese término para ese doc_id correspondiente. Sin embargo, si el doc_id del posting no está, inicializamos ese posting con el tf-idf correspondiente. Una vez creado el diccionario con las listas de postings se procede a realizar el ranking. El Heap de Ranking se encarga de la ordenación y el devolver hasta un cutoff definido, en el método orientado a términos solo vamos introduciendo todos los cosenos que hayamos calculado y también los paths de los documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3ti8qGedgNB"
   },
   "source": [
    "## Ejercicio 1.2: Método orientado a documentos* (1pt)\n",
    "\n",
    "Implementar el método orientado a documentos (con heap de postings) en una clase DocBasedVSMSearcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "wzZ-6OG0dvwX"
   },
   "outputs": [],
   "source": [
    "class DocBasedVSMSearcher(Searcher):\n",
    "    # Your new code here (exercise 1.2*) #\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xYd4hzhukr"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpXHr18Cdl2Q"
   },
   "source": [
    "## Ejercicio 1.3: Heap de ránking (0.5pt)\n",
    "\n",
    "Reimplementar la clase entregada SearchRanking para utilizar un heap de ránking (se recomienda usar el módulo [heapq](https://docs.python.org/3/library/heapq.html)). Nótese que esta opción se aprovecha mejor con la implementación orientada a documentos, aunque es compatible con la orientada a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "MOfT2yZGpMNi"
   },
   "outputs": [],
   "source": [
    "import heapq as heap\n",
    "class SearchRanking:\n",
    "    # TODO: to be implemented as heap (exercise 1.3) #\n",
    "    def __init__(self, cutoff):\n",
    "        self.ranking = [] # implementation as list, not as heap! TO BE MODIFIED\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def push(self, docid, score):\n",
    "        if self.ranking:\n",
    "            if len(self.ranking) < self.cutoff:\n",
    "                heap.heappush(self.ranking, (score, docid))\n",
    "            else:\n",
    "                min_elem = heap.heappop(self.ranking)\n",
    "\n",
    "                if score > min_elem[0]:\n",
    "                    heap.heappush(self.ranking, (score, docid))\n",
    "                else:\n",
    "                    heap.heappush(self.ranking, min_elem)\n",
    "        else:\n",
    "            heap.heappush(self.ranking, (score, docid))\n",
    "\n",
    "    def __iter__(self):\n",
    "        results = []\n",
    "        len_heap = len(self.ranking)\n",
    "        for num in range(self.cutoff):\n",
    "            if num < len_heap:\n",
    "                heap.heapify(self.ranking)\n",
    "                results.append(heap.heappop(self.ranking))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        sorted_results = sorted(results, key=lambda tup: tup[0], reverse=True)\n",
    "        return iter(sorted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJDzjUp-hwNZ"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "Para la implementación de esta clase se ha empleado la librería heapq como se nos recomendaba en el enunciado.\n",
    "\n",
    "La implementación ha constado de dos metodos, el método push que controla la inserción en el heap de ranking o no en función de si el score nuevo a introducir es mayor que el score minimo del heap o no (en el heap solo nos interesa guardar el top que nos indique cutoff hasta el momento). Y el metodo iter que reordena y saca los resultados finales del heap hasta que se vacíe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNkPcUjMpNRn"
   },
   "source": [
    "# Ejercicio 2: Índice en RAM (3pt)\n",
    "\n",
    "Implementar un índice propio que pueda hacer las mismas funciones que la implementación basada en Whoosh definida en la práctica 1. Como primera fase más sencilla, los índices se crearán completamente en RAM. Se guardarán a disco y leerán de disco en modo serializado (ver módulo [pickle](https://docs.python.org/3/library/pickle.html)).\n",
    "\n",
    "Para guardar el índice se utilizarán los nombres de fichero definidos por las variables estáticas de la clase Config. \n",
    "\n",
    "Antes de guardar el índice, se borrarán todos los ficheros que pueda haber creados en el directorio del índice. Asimismo, el directorio se creará si no estuviera creado, de forma que no haga falta crearlo a mano. Este detalle se hará igual en los siguientes ejercicios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVzsIg0Zev7a"
   },
   "source": [
    "## Ejercicio 2.1: Estructura de índice\n",
    "\n",
    "Implementar la clase RAMIndex como subclase de Index con las estructuras necesarias: diccionario, listas de postings, más la información que se necesite. \n",
    "\n",
    "Para este ejercicio en las listas de postings sólo será necesario guardar los docIDs y las frecuencias; no es necesario almacenar las posiciones de los términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "VqSKneeSe2bN"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "class RAMIndex(Index):\n",
    "    def __init__(self, index_path):\n",
    "        self.dict_postings = {}\n",
    "        super().__init__(index_path)\n",
    "        self.index_path = index_path\n",
    "    \n",
    "    def postings(self, term):\n",
    "        return self.dict_postings[term]\n",
    "    \n",
    "    def all_terms(self):\n",
    "        return list(self.dict_postings.keys())\n",
    "    \n",
    "    def save(self, path):\n",
    "        # guardamos lo basico de un indice\n",
    "        super().save(path)\n",
    "        \n",
    "        # guardamos diccionario de postings\n",
    "        p = os.path.join(path, Config.DICTIONARY_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.dict_postings, f) \n",
    "        \n",
    "        # guardamos el fichero de paths\n",
    "        p = os.path.join(path, Config.PATHS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.docmap, f)\n",
    "    \n",
    "    def open(self, path):\n",
    "        super().open(path)\n",
    "        try:\n",
    "            path_dict = os.path.join(path, Config.DICTIONARY_FILE)\n",
    "            with open(path_dict, 'rb') as file_dict:\n",
    "                self.dict_postings = pickle.load(file_dict)\n",
    "                \n",
    "            path_file = os.path.join(path, Config.PATHS_FILE)\n",
    "            with open(path_file, 'rb') as file_paths:\n",
    "                self.docmap = pickle.load(file_paths)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucORmwfCh4Um"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "Para la implementación del índice en RAM nos hemos basado en la implementación principal dada en las clases de teoría. Las claves del diccionario serán los términos y el valor de cada clave una lista que contiene tuplas con el siguiente formato (doc_id, frecuencia). Gracias a esta implementación, podemos sacar información del índice de forma bastante sencilla. La lista de postings para un término, será acceder al valor del diccionario mediante esa clave (término) y para obtener todos los términos devolvemos una lista con todas las claves del diccionario.\n",
    "\n",
    "Hemos tenido que implementar los métodos de save y open ya que a parte de los módulos, en este índice se trabaja con la lista de postings que se crea y con el docmap que contiene los paths de los documentos. Por lo tanto, en cada método se guardan/abren los módulos con la llamada al constructor de la clase padre y luego guardamos/abrimos lo que hemos construido con el Builder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqbc9ng8e28p"
   },
   "source": [
    "## Ejercicio 2.2 Construcción del índice\n",
    "\n",
    "Implementar la clase RAMIndexBuilder como subclase de Builder, que cree todo el índice en RAM a partir de una colección de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tHQ4UCf5pTw8"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "class RAMIndexBuilder(Builder):\n",
    "    def __init__(self, path, parser=BasicParser()):\n",
    "        if os.path.exists(path): shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "        self.path = path\n",
    "        self.parser = parser\n",
    "        self.index = RAMIndex(path)\n",
    "    \n",
    "    def index_document(self, path, text):\n",
    "        # sacamos el id del documento a indexar (se comienza en 0)\n",
    "        index_doc = self.index.ndocs()\n",
    "        # añadimos este nuevo documento al indice\n",
    "        self.index.add_doc(path)\n",
    "        \n",
    "        dterms = self.parser.parse(text)\n",
    "        aux_dict = {}\n",
    "        \n",
    "        for term in dterms:\n",
    "            if term in aux_dict.keys():\n",
    "                aux_dict[term] += 1\n",
    "            else:\n",
    "                aux_dict[term] = 1\n",
    "            \n",
    "        for term in aux_dict.keys():\n",
    "            if term in self.index.dict_postings.keys():\n",
    "                self.index.dict_postings[term].append((index_doc, aux_dict[term]))\n",
    "            else:\n",
    "                self.index.dict_postings[term] = []\n",
    "                self.index.dict_postings[term].append((index_doc, aux_dict[term]))\n",
    "                \n",
    "    def commit(self):\n",
    "        self.index.save(self.path) \n",
    "        file = open(self.path + Config.INDEX_FILE, \"wb\")\n",
    "        pickle.dump(self.index, file) \n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_mxRswZh74N"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "Para esta implementación hemos desarrollado de nuevo los métodos de 'commit' e 'index_document'. En el commit simplemente guardamos el índice con el formato de fichero que se especifica en las variables estáticas de la clase Config. La librería pickle nos ayuda a guardar a disco en modo serializado. Para la indexación de un documento nuevo lo añadimos al docmap, creamos un diccionario auxiliar con las frecuencias de todos los términos, y por último, creamos el diccionario con las listas de postings. Cada clave será un término y el valor una lista de tuplas de formato: (doc_id, frecuencia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lOWgbqZpV01"
   },
   "source": [
    "# Ejercicio 3: Índice en disco* (1pt)\n",
    "\n",
    "Reimplementar los índices definiendo las clases DiskIndex y DiskIndexBuilder de forma que:\n",
    "\n",
    "*\tEl índice se siga creando entero en RAM (por ejemplo, usando estructuras similares a las del ejercicio 2).\n",
    "*\tPero el índice se guarde en disco dato a dato (docIDs, frecuencias, etc.).\n",
    "*\tAl cargar el índice, sólo el diccionario se lee a RAM, y se accede a las listas de postings en disco cuando son necesarias (p.e. en tiempo de consulta).\n",
    "\n",
    "Se sugiere guardar el diccionario en un fichero y las listas de postings en otro, utilizando los nombres de fichero definidos como variables estáticas en la clase Config.\n",
    "\n",
    "Observación: se sugiere inicialmente guardar en disco las estructuras de índice en modo texto para poder depurar los programas. Una vez asegurada la corrección de los programas, puede ser más fácil pasar a modo binario o serializable (usando el módulo pickle como en ejercicios previos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "br7yqFrnpZYl"
   },
   "outputs": [],
   "source": [
    "class DiskIndex(Index):\n",
    "    \n",
    "    def __init__(self, index_path):\n",
    "        self.dict_postings = {}\n",
    "        self.dict_terms = {}\n",
    "        self.postings_path = os.path.join(index_path, Config.POSTINGS_FILE)\n",
    "        super().__init__(index_path)\n",
    "        self.index_path = index_path\n",
    "\n",
    "    def postings(self, term):\n",
    "        postings_list = []\n",
    "        with open(self.postings_path, 'r') as f:\n",
    "            info, postings = f.readline().strip().split(\"|\")\n",
    "            info = info.split(\" \")\n",
    "            postings = postings.split(\" \")\n",
    "\n",
    "            for i in range(0, int(info[1]) * 2, 2):\n",
    "                postings_list.append([int(postings[i]), int(postings[i+1])])\n",
    "\n",
    "        return postings_list\n",
    "\n",
    "    def all_terms(self):\n",
    "        return self.dict_postings.keys()\n",
    "\n",
    "    def doc_freq(self, term):\n",
    "        with open(self.postings_path, 'r') as f:\n",
    "            f.seek(self.dict_postings[term])\n",
    "            result = f.readline().split(\" \")[1]\n",
    "            \n",
    "        return int(result[0])\n",
    "\n",
    "    def save(self, path):\n",
    "        \n",
    "        with open(os.path.join(path, Config.PATHS_FILE), 'wb') as file_paths:\n",
    "            pickle.dump(self.docmap, file_paths)\n",
    "\n",
    "        with open(os.path.join(path, Config.POSTINGS_FILE), 'w') as file_postings:\n",
    "            for term, postings in self.dict_postings.items():\n",
    "                self.dict_postings[term] = file_postings.tell()\n",
    "                n_postings = len(postings)\n",
    "                file_postings.write(term + \" \" + str(n_postings) + \"|\")\n",
    "                \n",
    "                for p in postings:\n",
    "                    file_postings.write(str(p[0]) + \" \")\n",
    "                    file_postings.write(str(p[1]) + \" \")\n",
    "                    if term not in self.dict_terms.keys():\n",
    "                        self.dict_terms[term] = p[1]\n",
    "                    else:\n",
    "                        self.dict_terms[term] += p[1]\n",
    "                \n",
    "                file_postings.write(\"\\n\")\n",
    "\n",
    "        with open(os.path.join(path, Config.DICTIONARY_FILE), 'wb') as file_dict:\n",
    "            pickle.dump(self.dict_terms, file_dict)\n",
    "\n",
    "        super().save(path)\n",
    "\n",
    "    \n",
    "    def open(self, path):\n",
    "        super().open(path)\n",
    "        try:\n",
    "            with open(os.path.join(path, Config.PATHS_FILE), 'rb') as file_paths:\n",
    "                self.docmap = pickle.load(file_paths)\n",
    "\n",
    "            with open(os.path.join(path, Config.DICTIONARY_FILE), 'rb') as file_dict:\n",
    "                self.dict_postings = pickle.load(file_dict)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "class DiskIndexBuilder(Builder):\n",
    "    \n",
    "    def __init__(self, path, parser=BasicParser()):\n",
    "        if os.path.exists(path): shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "        self.path = path\n",
    "        self.parser = parser\n",
    "        self.index = DiskIndex(path)\n",
    "\n",
    "    def index_document(self, path, text):\n",
    "        # sacamos el id del documento a indexar (se comienza en 0)\n",
    "        index_doc = self.index.ndocs()\n",
    "        # añadimos este nuevo documento al indice\n",
    "        self.index.add_doc(path)\n",
    "\n",
    "        dterms = self.parser.parse(text)\n",
    "        aux_dict = {}\n",
    "        for term in dterms:\n",
    "            if term in aux_dict.keys():\n",
    "                aux_dict[term] += 1\n",
    "            else:\n",
    "                aux_dict[term] = 1\n",
    "            \n",
    "        for term in aux_dict.keys():\n",
    "            if term in self.index.dict_postings.keys():\n",
    "                self.index.dict_postings[term].append((index_doc, aux_dict[term]))\n",
    "            else:\n",
    "                self.index.dict_postings[term] = []\n",
    "                self.index.dict_postings[term].append((index_doc, aux_dict[term]))\n",
    "    \n",
    "    \n",
    "    def commit(self):\n",
    "        self.index.save(self.path) \n",
    "        with open(self.path + Config.INDEX_FILE, \"wb\") as file_index:\n",
    "            pickle.dump(self.index, file_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzTje0viiM9I"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "La implementación de esta clase ha sido muy similar a la del ejercicio anterior, la única diferencia es que ahora la lista de postings no se guarda en RAM si no que solo guardamos el diccionario con los terminos y sus frecuencias. Por tanto a la hora de obtener los postings accediendo al diccionario de postings, este no habrá sido previamente cargado de lo que teniamos en RAM sino del contenido en disco.\n",
    "\n",
    "A partir de la colección de urls no llega a funcionar, pero para los ejemplos de toy, sí. En el main está comentado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcxuclLwpaM-"
   },
   "source": [
    "# Ejercicio 4: Motor de búsqueda proximal* (1pt)\n",
    "\n",
    "Implementar un método de búsqueda proximal en una clase ProximitySearcher, utilizando las interfaces de índices posicionales. Igual que en los ejercicios anteriores, se sugiere definir esta clase como subclase (directa o indirecta) de Searcher. Para empezar a probar este buscador, se proporciona una implementación de indexación posicional basada en Whoosh (WhooshPositionalIndex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "e3uq565SpfSA"
   },
   "outputs": [],
   "source": [
    "class ProximitySearcher(Searcher):\n",
    "    # Your new code here (exercise 4*) #\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d4hWTstiOIT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPPWV7pepf85"
   },
   "source": [
    "# Ejercicio 5: Índice posicional* (1pt)\n",
    "\n",
    "Implementar una variante adicional de índice (como subclase si se considera oportuno) que extienda las estructuras de índices con la inclusión de posiciones en las listas de postings. La implementación incluirá una clase PositionalIndexBuilder para la construcción del índice posicional así como una clase PositionalIndex para proporcionar acceso al mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "zg8MIMpipih1"
   },
   "outputs": [],
   "source": [
    "class PositionalIndex(RAMIndex):\n",
    "    def __init__(self, path):\n",
    "        self.dict_postings = {}\n",
    "        super().__init__(path)\n",
    "    \n",
    "    def postings(self, term):\n",
    "        return [(item[0], len(item[1])) for item in self.dict_postings[term]]\n",
    "    \n",
    "    def positional_postings(self, term):\n",
    "        return [(item[0], item[1]) for item in self.dict_postings[term]]\n",
    "            \n",
    "\n",
    "class PositionalIndexBuilder(RAMIndexBuilder):\n",
    "    def __init__(self, path, parser=BasicParser()):\n",
    "        super().__init__(path)\n",
    "        self.index = PositionalIndex(path)\n",
    "        \n",
    "    def index_document(self, path, text):\n",
    "        # sacamos el id del documento a indexar (se comienza en 0)\n",
    "        index_doc = self.index.ndocs()\n",
    "        # añadimos este nuevo documento al indice\n",
    "        self.index.add_doc(path)\n",
    "        \n",
    "        dterms = self.parser.parse(text)     \n",
    "        aux_dict = {}\n",
    "        for position, term in enumerate(dterms):\n",
    "            if term in aux_dict.keys():\n",
    "                aux_dict[term].append(position)\n",
    "            else:\n",
    "                aux_dict[term] = []\n",
    "                aux_dict[term].append(position)\n",
    "            \n",
    "        for term in aux_dict.keys():\n",
    "            if term in self.index.dict_postings.keys():\n",
    "                self.index.dict_postings[term].append((index_doc, aux_dict[term]))\n",
    "            else:\n",
    "                self.index.dict_postings[term] = []\n",
    "                self.index.dict_postings[term].append((index_doc, aux_dict[term]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1gukouXiPV3"
   },
   "source": [
    "### Explicación/documentación, indicando además el tipo de índice que se ha implementado y los aspectos que sean destacables\n",
    "\n",
    "Hemos implementado el índice posicional como subclase de RAMIndex ya que en este índice se guarda la información en el mismo formato y lo que se guarda es muy parecido a lo implementado ya. Lo único que cambia del índice es que debemos implementar el método positional_postings para devolver la lista de postings de un término en el formato posicional (esto se explicará mas adelante) y también el método postings para devolver la lista en el formato tradicional: [(doc_id, frecuencia), (doc_id, frecuencia), ...].\n",
    "\n",
    "Lo único que cambia al construir el índice sería la forma en la que se indexa un documento. En este caso, en vez de guardar la frecuencia, se va a guardar una lista con las posiciones en las que se encuentra el término dentro del documento. El formato dado un término sería el siguiente: 'aa' [(0, [0, 3, 9, 15]), (3, [6, 10, 16])]. Se crea un diccionario el cual cada clave contiene el término y el valor de la clave es una lista  de tuplas con el formato del ejemplo que se ha puesto; este formato explicado sería(doc_id, lista_posiciones): el término 'aa' se localiza en las posiciones 0, 3, 9 y 15 del documento con id = 0 y en las posiciones 6, 10 y 16 del documento con doc_id = 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6HbYGn8pjKZ"
   },
   "source": [
    "# Ejercicio 6: PageRank (1pt)\n",
    "\n",
    "Implementar el algoritmo PageRank en una clase PagerankDocScorer, que permitirá devolver un ranking de los documentos de manera similar a como hace un Searcher (pero sin recibir una consulta). \n",
    "\n",
    "Se recomienda, al menos inicialmente, llevar a cabo una implementación con la que los valores de PageRank sumen 1, para ayudar a la validación de la misma. Posteriormente, si se desea, se pueden escalar (o no, a criterio del estudiante) los cálculos omitiendo la división por el número total de páginas en el grafo. Será necesario tratar los nodos sumidero tal como se ha explicado en las clases de teoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "_aQE7SBgpk1S"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "class PagerankDocScorer():\n",
    "    def __init__(self, graphfile, r, n_iter):\n",
    "        self.r = r\n",
    "        self.n_iter = n_iter\n",
    "        file = open(graphfile, \"r\")\n",
    "        lines = file.read().splitlines()\n",
    "        self.pageRank_dict = {}\n",
    "        for line in lines:\n",
    "            links = line.split()\n",
    "            if links[0] in self.pageRank_dict.keys():\n",
    "                self.pageRank_dict[links[0]][2].append(links[1])\n",
    "            else: \n",
    "                self.pageRank_dict[links[0]] = [0.0, 0.0, []]\n",
    "                self.pageRank_dict[links[0]][2].append(links[1])\n",
    "        \n",
    "        list_sumideros = []\n",
    "        for nodo in self.pageRank_dict.keys():\n",
    "            for elem in self.pageRank_dict[nodo][2]:\n",
    "                if elem not in self.pageRank_dict.keys():\n",
    "                    list_sumideros.append(elem)\n",
    "         \n",
    "        for sumidero in list_sumideros:\n",
    "            self.pageRank_dict[sumidero] = [0.0, 0.0, []]            \n",
    "                    \n",
    "        self.nodos = len(self.pageRank_dict.keys())\n",
    "        for nodo in self.pageRank_dict.keys():\n",
    "            self.pageRank_dict[nodo][0] = 1/self.nodos\n",
    "        \n",
    "        \n",
    "    def rank(self, cutoff):\n",
    "        for i in range(self.n_iter):\n",
    "            for nodo1 in self.pageRank_dict.keys():\n",
    "                nodos_from = []\n",
    "                for nodo2 in self.pageRank_dict.keys():\n",
    "                    if nodo1 in self.pageRank_dict[nodo2][2]:\n",
    "                        nodos_from.append(nodo2)\n",
    "                        \n",
    "                self.pageRank_dict[nodo1][1] = self.pageRank_dict[nodo1][0]\n",
    "                \n",
    "                suma = 0\n",
    "                for nodo in nodos_from: \n",
    "                    suma += self.pageRank_dict[nodo][1]/len(self.pageRank_dict[nodo][2])\n",
    "                \n",
    "                for nodo in self.pageRank_dict.keys():\n",
    "                    if len(self.pageRank_dict[nodo][2]) == 0:\n",
    "                        suma += self.pageRank_dict[nodo][1] / self.nodos \n",
    "                            \n",
    "                \n",
    "                self.pageRank_dict[nodo1][0] = (self.r/self.nodos) + (1-self.r)*suma\n",
    "        \n",
    "        sorted_list = sorted(self.pageRank_dict.items(), key=lambda item: item[1][0], reverse=True)\n",
    "        ranking = [(elem[0], elem[1][0]) for elem in sorted_list[:cutoff]]\n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ZT7seCiQiT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "Para la implementación del PageRank hemos tomado la decisión de crear un diccionario donde cada clave se corresponde a cada nodo y asociado a cada clave se corresponde una lista de tres elementos donde el primer elemento guarda el valor del PageRank actual de ese nodo, el segundo elemento guarda el PageRank anterior de ese nodo y el último elemento guarda una lista con los enlaces salientes del nodo.\n",
    "\n",
    "El método rank consiste en cumplir la función de cálculo del PageRank, la cual ha sido vista tanto en teoría como en las explicaciones de prácticas y también fijándonos en el algoritmo proporcionado en la explicación de la práctica.\n",
    "\n",
    "El primer bucle nos indica el número de iteraciones a realizar el cálculo del PageRank ya que no se ha implementado la convergencia como mecanismo de parada. En los dos bucles posteriores calculamos la lista de nodos que tienen algún enlace saliente a cada nodo para poder asi obtener su PageRank. Por último aplicamos la fórmula con los datos ya obtenidos, ordenamos el diccionario en base al último score calculado, o lo que es lo mismo, score actual(primera posición en la lista) y devolvemos los resultados en forma de lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zXhPtFzon72"
   },
   "source": [
    "# Programa de prueba **main**\n",
    "\n",
    "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta **collections** en el mismo directorio que este *notebook*. El fichero <u>toys.zip</u> hay que descomprimirlo para indexar las carpetas que contiene. Igualmente, el fichero <u>graphs.zip</u> incluye ficheros (*1k-links.dat*, *toy-graph1.dat*, *toy-graph2.dat*) que se deben descomprimir en la carpeta collections para que el main funcione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9MDo81ZmK9A"
   },
   "source": [
    "## Función **main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "fTdDacCRn0u6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Testing indices and search on ./collections/toys/toy1/\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toys/toy1/\n",
      "Done ( 0.019369125366210938 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toys/toy1/\n",
      "Done ( 0.010722637176513672 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toys/toy1/\n",
      "Done ( 0.008970022201538086 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toys/toy1/\n",
      "Done ( 0.0008933544158935547 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toys/toy1/\n",
      "Done ( 0.0035543441772460938 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toys/toy1/\n",
      "Done ( 0.0009160041809082031 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toys/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0012612342834472656 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toys/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0014290809631347656 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toys/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0005183219909667969 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toys/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0015277862548828125 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toys/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.00014472007751464844 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa dd\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0006232261657714844 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0005555152893066406 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0005500316619873047 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "./collections/toys/toy1/d2.txt \t 1.0\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0010929107666015625 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "./collections/toys/toy1/d2.txt \t 1.0\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0010776519775390625 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "./collections/toys/toy1/d2.txt \t 1.0\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0014138221740722656 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "./collections/toys/toy1/d2.txt \t 1.0\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0010802745819091797 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "./collections/toys/toy1/d2.txt \t 1.0\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0014824867248535156 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "./collections/toys/toy1/d2.txt \t 1.0\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0010313987731933594 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "./collections/toys/toy1/d2.txt \t 1.0\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.00015044212341308594 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "./collections/toys/toy1/d2.txt \t 1.0\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.00024175643920898438 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "./collections/toys/toy1/d2.txt \t 1.0\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0001590251922607422 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "./collections/toys/toy1/d2.txt \t 1.0\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0001380443572998047 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toys/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toys/toy1/d3.txt\n",
      "\n",
      "Done ( 0.001691579818725586 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toys/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toys/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0016558170318603516 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toys/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toys/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0015549659729003906 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa'\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0004892349243164062 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa'\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0009334087371826172 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0009853839874267578 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.00023031234741210938 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.001726388931274414 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.0002181529998779297 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa'\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.000102996826171875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa'\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 9.298324584960938e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa'\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 0.00010204315185546875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa'\n",
      "./collections/toys/toy1/d1.txt \t 0.7427813527082074\n",
      "./collections/toys/toy1/d3.txt \t 0.5773502691896258\n",
      "\n",
      "Done ( 9.322166442871094e-05 seconds )\n",
      "\n",
      "=================================================================\n",
      "Testing indices and search on ./collections/toys/toy2/\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toys/toy2/\n",
      "Done ( 0.007462024688720703 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toys/toy2/\n",
      "Done ( 0.007932186126708984 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toys/toy2/\n",
      "Done ( 0.007645845413208008 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toys/toy2/\n",
      "Done ( 0.0007483959197998047 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toys/toy2/\n",
      "Done ( 0.003110647201538086 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toys/toy2/\n",
      "Done ( 0.0007419586181640625 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toys/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0011267662048339844 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toys/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0010042190551757812 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toys/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0014185905456542969 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toys/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.00013518333435058594 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toys/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.00013327598571777344 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toys/toy2/example.txt\n",
      "\n",
      "Done ( 0.0011782646179199219 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toys/toy2/example.txt\n",
      "\n",
      "Done ( 0.0011434555053710938 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toys/toy2/example.txt\n",
      "\n",
      "Done ( 0.0010733604431152344 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.0004611015319824219 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.001270294189453125 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.0011332035064697266 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.0006556510925292969 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.0009474754333496094 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.0008132457733154297 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.02556544296910982\n",
      "\n",
      "Done ( 0.00010442733764648438 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.02556544296910982\n",
      "\n",
      "Done ( 9.655952453613281e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.02556544296910982\n",
      "\n",
      "Done ( 0.00010251998901367188 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.02556544296910982\n",
      "\n",
      "Done ( 0.002398967742919922 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toys/toy2/example.txt\n",
      "\n",
      "Done ( 0.001127004623413086 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toys/toy2/example.txt\n",
      "\n",
      "Done ( 0.0010745525360107422 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toys/toy2/example.txt\n",
      "\n",
      "Done ( 0.0010669231414794922 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.0004589557647705078 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.00142669677734375 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.00092315673828125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.0003123283386230469 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.0012664794921875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.036794545335038994\n",
      "\n",
      "Done ( 0.00042366981506347656 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.02556544296910982\n",
      "\n",
      "Done ( 0.00010323524475097656 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.02556544296910982\n",
      "\n",
      "Done ( 9.489059448242188e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.02556544296910982\n",
      "\n",
      "Done ( 0.0001010894775390625 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "./collections/toys/toy2/example.txt \t 0.902184145803128\n",
      "./collections/toys/toy2/hamlet.txt \t 0.02556544296910982\n",
      "\n",
      "Done ( 9.5367431640625e-05 seconds )\n",
      "\n",
      "=================================================================\n",
      "Testing indices and search on ./collections/urls.txt\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/urls.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ( 3.3373589515686035 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 3.0036323070526123 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.665405035018921 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 1.8450298309326172 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.1475260257720947 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 1.8579277992248535 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5899\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 23.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.015758752822875977 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5899\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 23.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.01546788215637207 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5899\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 23.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.01571965217590332 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5828\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 26 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.0003867149353027344 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5828\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 26 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.0014557838439941406 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for information probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.9288499170082907 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.4707338357114654 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.087930188843972 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0036046504974365234 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "2.9288499170082907 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.4707338357114654 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.087930188843972 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0034151077270507812 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "2.9288499170082907 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.4707338357114654 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.087930188843972 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003514528274536133 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.0007970333099365234 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.0011742115020751953 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.0025887489318847656 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.001184225082397461 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.001470327377319336 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.0012021064758300781 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.020445394707116333\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017287311957707156\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009502783841385028\n",
      "\n",
      "Done ( 0.000164031982421875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.020445394707116333\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017287311957707156\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009502783841385028\n",
      "\n",
      "Done ( 0.0016634464263916016 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.020445394707116333\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017287311957707156\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009502783841385028\n",
      "\n",
      "Done ( 0.0008566379547119141 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.020445394707116333\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017287311957707156\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009502783841385028\n",
      "\n",
      "Done ( 0.00015306472778320312 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for probability information\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.9288499170082907 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.4707338357114654 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.087930188843972 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003339052200317383 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "2.9288499170082907 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.4707338357114654 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.087930188843972 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003381967544555664 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "2.9288499170082907 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.4707338357114654 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.087930188843972 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0035049915313720703 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.0007884502410888672 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.00043272972106933594 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.003303050994873047 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.001384735107421875 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.0015530586242675781 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.0005609989166259766 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.020445394707116333\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017287311957707156\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009502783841385028\n",
      "\n",
      "Done ( 0.0001678466796875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.020445394707116333\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017287311957707156\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009502783841385028\n",
      "\n",
      "Done ( 0.00015282630920410156 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.020445394707116333\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017287311957707156\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009502783841385028\n",
      "\n",
      "Done ( 0.0009844303131103516 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.020445394707116333\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017287311957707156\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009502783841385028\n",
      "\n",
      "Done ( 0.000152587890625 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for higher probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.9090890464982255 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.5277784960384073 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7126723401480048 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004130840301513672 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "2.9090890464982255 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.5277784960384073 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7126723401480048 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0033614635467529297 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "2.9090890464982255 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.5277784960384073 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7126723401480048 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0035467147827148438 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.030784360982896303\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.012204249189851296\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.0056963841409530605\n",
      "\n",
      "Done ( 0.0012691020965576172 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.030784360982896303\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.012204249189851296\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.0056963841409530605\n",
      "\n",
      "Done ( 0.0010004043579101562 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.030784360982896303\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.012204249189851296\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.0056963841409530605\n",
      "\n",
      "Done ( 0.002506732940673828 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.030784360982896303\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.012204249189851296\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.0056963841409530605\n",
      "\n",
      "Done ( 0.0012209415435791016 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.030784360982896303\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.012204249189851296\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.0056963841409530605\n",
      "\n",
      "Done ( 0.0014722347259521484 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.030784360982896303\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.012204249189851296\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.0056963841409530605\n",
      "\n",
      "Done ( 0.0011794567108154297 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.03037611486868911\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.01210356128014145\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.005677853953970711\n",
      "\n",
      "Done ( 0.0001690387725830078 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.03037611486868911\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.01210356128014145\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.005677853953970711\n",
      "\n",
      "Done ( 0.002002239227294922 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.03037611486868911\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.01210356128014145\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.005677853953970711\n",
      "\n",
      "Done ( 0.00016450881958007812 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.03037611486868911\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.01210356128014145\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.005677853953970711\n",
      "\n",
      "Done ( 0.00015211105346679688 seconds )\n",
      "\n",
      "----------------------------\n",
      "Testing index performance on ./collections/urls.txt document collection\n",
      "  Build time...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tWhooshIndex: 2.3596839904785156 seconds ---\n",
      "\tWhooshForwardIndex: 2.671544075012207 seconds ---\n",
      "\tWhooshPositionalIndex: 2.8233237266540527 seconds ---\n",
      "\tRAMIndex: 1.970839262008667 seconds ---\n",
      "  Load time...\n",
      "\tWhooshIndex: 0.0013935565948486328 seconds ---\n",
      "\tWhooshForwardIndex: 0.0010342597961425781 seconds ---\n",
      "\tWhooshPositionalIndex: 0.0010662078857421875 seconds ---\n",
      "\tRAMIndex: 0.003216266632080078 seconds ---\n",
      "  Disk space...\n",
      "\tWhooshIndex: 1060414 space ---\n",
      "\tWhooshForwardIndex: 1134673 space ---\n",
      "\tWhooshPositionalIndex: 1212449 space ---\n",
      "\tRAMIndex: 123609 space ---\n",
      "----------------------------\n",
      "Testing search performance on ./collections/urls.txt document collection with query: 'information probability'\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.9288499170082907 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.4707338357114654 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.087930188843972 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0037107467651367188 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.005396842956542969 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.0010902881622314453 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.001138448715209961 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.001245737075805664 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.0012924671173095703 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.020445394707116333\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017287311957707156\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009502783841385028\n",
      "\n",
      "Done ( 0.00016188621520996094 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.00020575523376464844 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ./collections/urls.txt document collection with query: 'probability information'\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.9288499170082907 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.4707338357114654 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.087930188843972 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0037910938262939453 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.005532264709472656 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.0008358955383300781 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0008957386016845703 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.02072017484205769\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017431122796949937\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009533797031026966\n",
      "\n",
      "Done ( 0.0014584064483642578 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.001504659652709961 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.020445394707116333\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.017287311957707156\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.009502783841385028\n",
      "\n",
      "Done ( 0.00016021728515625 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.0002052783966064453 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ./collections/urls.txt document collection with query: 'higher probability'\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.9090890464982255 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.5277784960384073 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7126723401480048 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004973649978637695 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.006972789764404297 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.030784360982896303\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.012204249189851296\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.0056963841409530605\n",
      "\n",
      "Done ( 0.0010628700256347656 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0011246204376220703 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.030784360982896303\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.012204249189851296\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.0056963841409530605\n",
      "\n",
      "Done ( 0.002290487289428711 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.002348184585571289 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "https://en.wikipedia.org/wiki/Simpson's_paradox \t 0.03037611486868911\n",
      "https://en.wikipedia.org/wiki/Entropy \t 0.01210356128014145\n",
      "https://en.wikipedia.org/wiki/Bias \t 0.005677853953970711\n",
      "\n",
      "Done ( 0.00020766258239746094 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.0002658367156982422 seconds ---\n",
      "=================================================================\n",
      "Testing indices and search on ./collections/docs1k.zip\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 80.36672973632812 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 93.56473183631897 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 89.99328708648682 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 61.78673839569092 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 225.7540202140808 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 59.415568113327026 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 118003\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(0, 28), (9, 4)]\n",
      "Done ( 0.27117466926574707 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 118003\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(0, 28), (9, 4)]\n",
      "Done ( 0.27285146713256836 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 118003\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(0, 28), (9, 4)]\n",
      "Done ( 0.25925374031066895 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 111590\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1406 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(0, 28), (9, 4)]\n",
      "Done ( 0.0045163631439208984 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 111590\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1406 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(0, 28), (9, 4)]\n",
      "Done ( 0.0091400146484375 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for obama family tree\n",
      "  WhooshSearcher with index WhooshIndex for query 'obama family tree'\n",
      "16.485499731264426 \t clueweb09-en0010-79-2218.html\n",
      "15.887791854254225 \t clueweb09-en0010-57-32937.html\n",
      "15.808952372971135 \t clueweb09-en0001-02-21241.html\n",
      "15.611476287511962 \t clueweb09-en0008-45-29117.html\n",
      "15.554087101921635 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.03789258003234863 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "16.485499731264426 \t clueweb09-en0010-79-2218.html\n",
      "15.887791854254225 \t clueweb09-en0010-57-32937.html\n",
      "15.808952372971135 \t clueweb09-en0001-02-21241.html\n",
      "15.611476287511962 \t clueweb09-en0008-45-29117.html\n",
      "15.554087101921635 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.03775501251220703 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "16.485499731264426 \t clueweb09-en0010-79-2218.html\n",
      "15.887791854254225 \t clueweb09-en0010-57-32937.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.808952372971135 \t clueweb09-en0001-02-21241.html\n",
      "15.611476287511962 \t clueweb09-en0008-45-29117.html\n",
      "15.554087101921635 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.05137205123901367 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.28421789364376177\n",
      "clueweb09-en0009-30-2768.html \t 0.22631966869496392\n",
      "clueweb09-en0001-02-21241.html \t 0.224801913947326\n",
      "clueweb09-en0009-30-2441.html \t 0.22386305596546352\n",
      "clueweb09-en0009-30-2755.html \t 0.22349064069479563\n",
      "\n",
      "Done ( 1.9562327861785889 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.28421789364376177\n",
      "clueweb09-en0009-30-2768.html \t 0.22631966869496392\n",
      "clueweb09-en0001-02-21241.html \t 0.224801913947326\n",
      "clueweb09-en0009-30-2441.html \t 0.22386305596546352\n",
      "clueweb09-en0009-30-2755.html \t 0.22349064069479563\n",
      "\n",
      "Done ( 0.012063980102539062 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.28421789364376177\n",
      "clueweb09-en0009-30-2768.html \t 0.22631966869496392\n",
      "clueweb09-en0001-02-21241.html \t 0.224801913947326\n",
      "clueweb09-en0009-30-2441.html \t 0.22386305596546352\n",
      "clueweb09-en0009-30-2755.html \t 0.22349064069479563\n",
      "\n",
      "Done ( 0.49008607864379883 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.28421789364376177\n",
      "clueweb09-en0009-30-2768.html \t 0.22631966869496392\n",
      "clueweb09-en0001-02-21241.html \t 0.224801913947326\n",
      "clueweb09-en0009-30-2441.html \t 0.22386305596546352\n",
      "clueweb09-en0009-30-2755.html \t 0.22349064069479563\n",
      "\n",
      "Done ( 0.01302480697631836 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.28421789364376177\n",
      "clueweb09-en0009-30-2768.html \t 0.22631966869496392\n",
      "clueweb09-en0001-02-21241.html \t 0.224801913947326\n",
      "clueweb09-en0009-30-2441.html \t 0.22386305596546352\n",
      "clueweb09-en0009-30-2755.html \t 0.22349064069479563\n",
      "\n",
      "Done ( 3.1261892318725586 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.28421789364376177\n",
      "clueweb09-en0009-30-2768.html \t 0.22631966869496392\n",
      "clueweb09-en0001-02-21241.html \t 0.224801913947326\n",
      "clueweb09-en0009-30-2441.html \t 0.22386305596546352\n",
      "clueweb09-en0009-30-2755.html \t 0.22349064069479563\n",
      "\n",
      "Done ( 0.013788461685180664 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.2855798492804466\n",
      "clueweb09-en0001-02-21241.html \t 0.23011119798598423\n",
      "clueweb09-en0009-30-2768.html \t 0.2236230958616016\n",
      "clueweb09-en0009-30-2441.html \t 0.2235911398481371\n",
      "clueweb09-en0009-30-2755.html \t 0.2232891585409881\n",
      "\n",
      "Done ( 0.027341842651367188 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.2855798492804466\n",
      "clueweb09-en0001-02-21241.html \t 0.23011119798598423\n",
      "clueweb09-en0009-30-2768.html \t 0.2236230958616016\n",
      "clueweb09-en0009-30-2441.html \t 0.2235911398481371\n",
      "clueweb09-en0009-30-2755.html \t 0.2232891585409881\n",
      "\n",
      "Done ( 0.0026509761810302734 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.2855798492804466\n",
      "clueweb09-en0001-02-21241.html \t 0.23011119798598423\n",
      "clueweb09-en0009-30-2768.html \t 0.2236230958616016\n",
      "clueweb09-en0009-30-2441.html \t 0.2235911398481371\n",
      "clueweb09-en0009-30-2755.html \t 0.2232891585409881\n",
      "\n",
      "Done ( 0.1741940975189209 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.2855798492804466\n",
      "clueweb09-en0001-02-21241.html \t 0.23011119798598423\n",
      "clueweb09-en0009-30-2768.html \t 0.2236230958616016\n",
      "clueweb09-en0009-30-2441.html \t 0.2235911398481371\n",
      "clueweb09-en0009-30-2755.html \t 0.2232891585409881\n",
      "\n",
      "Done ( 0.02557969093322754 seconds )\n",
      "\n",
      "----------------------------\n",
      "Testing index performance on ./collections/docs1k.zip document collection\n",
      "  Build time...\n",
      "\tWhooshIndex: 80.68433427810669 seconds ---\n",
      "\tWhooshForwardIndex: 87.33258485794067 seconds ---\n",
      "\tWhooshPositionalIndex: 84.46648454666138 seconds ---\n",
      "\tRAMIndex: 54.119834423065186 seconds ---\n",
      "  Load time...\n",
      "\tWhooshIndex: 0.008711576461791992 seconds ---\n",
      "\tWhooshForwardIndex: 0.00783395767211914 seconds ---\n",
      "\tWhooshPositionalIndex: 0.010146379470825195 seconds ---\n",
      "\tRAMIndex: 0.2619786262512207 seconds ---\n",
      "  Disk space...\n",
      "\tWhooshIndex: 22819862 space ---\n",
      "\tWhooshForwardIndex: 28575696 space ---\n",
      "\tWhooshPositionalIndex: 31237566 space ---\n",
      "\tRAMIndex: 5967296 space ---\n",
      "----------------------------\n",
      "Testing search performance on ./collections/docs1k.zip document collection with query: 'obama family tree'\n",
      "  WhooshSearcher with index WhooshIndex for query 'obama family tree'\n",
      "16.485499731264426 \t clueweb09-en0010-79-2218.html\n",
      "15.887791854254225 \t clueweb09-en0010-57-32937.html\n",
      "15.808952372971135 \t clueweb09-en0001-02-21241.html\n",
      "15.611476287511962 \t clueweb09-en0008-45-29117.html\n",
      "15.554087101921635 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.05255246162414551 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.0607905387878418 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.28421789364376177\n",
      "clueweb09-en0009-30-2768.html \t 0.22631966869496392\n",
      "clueweb09-en0001-02-21241.html \t 0.224801913947326\n",
      "clueweb09-en0009-30-2441.html \t 0.22386305596546352\n",
      "clueweb09-en0009-30-2755.html \t 0.22349064069479563\n",
      "\n",
      "Done ( 1.1959452629089355 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 1.195988416671753 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.28421789364376177\n",
      "clueweb09-en0009-30-2768.html \t 0.22631966869496392\n",
      "clueweb09-en0001-02-21241.html \t 0.224801913947326\n",
      "clueweb09-en0009-30-2441.html \t 0.22386305596546352\n",
      "clueweb09-en0009-30-2755.html \t 0.22349064069479563\n",
      "\n",
      "Done ( 0.012129068374633789 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.012827634811401367 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "clueweb09-en0010-79-2218.html \t 0.2855798492804466\n",
      "clueweb09-en0001-02-21241.html \t 0.23011119798598423\n",
      "clueweb09-en0009-30-2768.html \t 0.2236230958616016\n",
      "clueweb09-en0009-30-2441.html \t 0.2235911398481371\n",
      "clueweb09-en0009-30-2755.html \t 0.2232891585409881\n",
      "\n",
      "Done ( 0.0016179084777832031 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.0016579627990722656 seconds ---\n",
      "=================================================================\n",
      "Testing indices and search on ./collections/docs10k.zip\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/docs10k.zip\n",
      "Done ( 552.4117367267609 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/docs10k.zip\n",
      "Done ( 586.3103792667389 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/docs10k.zip\n",
      "Done ( 570.350527048111 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/docs10k.zip\n",
      "Done ( 454.83661580085754 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/docs10k.zip\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3432/1334759539.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Pagerank with simulated links for doc1k %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3432/1334759539.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest_collection\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcollections_root_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"urls.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_root_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"urls/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wikipedia\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"information probability\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"probability information\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"higher probability\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtest_collection\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcollections_root_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"docs1k.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_root_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"docs1k/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"obama family tree\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtest_collection\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcollections_root_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"docs10k.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_root_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"docs10k/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"obama family tree\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtest_pagerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./collections/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3432/1334759539.py\u001b[0m in \u001b[0;36mtest_collection\u001b[0;34m(collection_path, index_path, word, queries, analyse_performance)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtest_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhooshPositionalBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"whoosh_pos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtest_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAMIndexBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"ram\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtest_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDiskIndexBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"disk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtest_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPositionalIndexBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3432/1334759539.py\u001b[0m in \u001b[0;36mtest_build\u001b[0;34m(builder, collection)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# we can also save any extra information we may need\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# (and that cannot be computed until the entire collection is scanned/indexed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done (\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seconds )\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3432/3088631760.py\u001b[0m in \u001b[0;36mcommit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINDEX_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3432/3088631760.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3432/2156061046.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodulemap\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNORMS_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3432/2156061046.py\u001b[0m in \u001b[0;36mcompute_modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdocid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdocid\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodulemap\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodulemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodulemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midf_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdocid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodulemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodulemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdocid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodulemap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    index_root_dir = \"./index/\"\n",
    "    collections_root_dir = \"./collections/\"\n",
    "    test_collection (collections_root_dir + \"toys/toy1/\", index_root_dir + \"toys/toy1/\", \"cc\", [\"aa dd\", \"aa\"], False)\n",
    "    test_collection (collections_root_dir + \"toys/toy2/\", index_root_dir + \"toys/toy2/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "    test_collection (collections_root_dir + \"urls.txt\", index_root_dir + \"urls/\", \"wikipedia\", [\"information probability\", \"probability information\", \"higher probability\"], True)\n",
    "    test_collection (collections_root_dir + \"docs1k.zip\", index_root_dir + \"docs1k/\", \"seat\", [\"obama family tree\"], True)\n",
    "    test_collection (collections_root_dir + \"docs10k.zip\", index_root_dir + \"docs10k/\", \"seat\", [\"obama family tree\"], False)\n",
    "    test_pagerank(\"./collections/\", 5)\n",
    "\n",
    "def test_collection(collection_path: str, index_path: str, word: str, queries: list, analyse_performance: bool):\n",
    "    print(\"=================================================================\")\n",
    "    print(\"Testing indices and search on \" + collection_path)\n",
    "\n",
    "    # We now test building different implementations of an index\n",
    "    test_build(WhooshBuilder(index_path + \"whoosh\"), collection_path)\n",
    "    test_build(WhooshForwardBuilder(index_path + \"whoosh_fwd\"), collection_path)\n",
    "    test_build(WhooshPositionalBuilder(index_path + \"whoosh_pos\"), collection_path)\n",
    "    test_build(RAMIndexBuilder(index_path + \"ram\"), collection_path)\n",
    "    test_build(DiskIndexBuilder(index_path + \"disk\"), collection_path)\n",
    "    test_build(PositionalIndexBuilder(index_path + \"pos\"), collection_path)\n",
    "\n",
    "    # We now inspect all the implementations\n",
    "    indices = [\n",
    "            WhooshIndex(index_path + \"whoosh\"),\n",
    "            WhooshForwardIndex(index_path + \"whoosh_fwd\"), \n",
    "            WhooshPositionalIndex(index_path + \"whoosh_pos\"), \n",
    "            RAMIndex(index_path + \"ram\"),\n",
    "            # DiskIndex(index_path + \"disk\"),\n",
    "            PositionalIndex(index_path + \"pos\"),\n",
    "            ]\n",
    "    for index in indices:\n",
    "        test_read(index, word)\n",
    "\n",
    "    for query in queries:\n",
    "        print(\"------------------------------\")\n",
    "        print(\"Checking search results for %s\" % (query))\n",
    "        # Whoosh searcher can only work with its own indices\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh\"), WhooshIndex(index_path + \"whoosh\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_fwd\"), WhooshForwardIndex(index_path + \"whoosh_fwd\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_pos\"), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        # test_search(ProximitySearcher(WhooshPositionalIndex(index_path + \"whoosh_pos\")), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        for index in indices:\n",
    "            # our searchers should work with any other index\n",
    "            test_search(SlowVSMSearcher(index), index, query, 5)\n",
    "            test_search(TermBasedVSMSearcher(index), index, query, 5)\n",
    "            # test_search(DocBasedVSMSearcher(index), index, query, 5)\n",
    "            # test_search(ProximitySearcher(PositionalIndex(index_path + \"pos\")), PositionalIndex(index_path + \"pos\"), query, 5)\n",
    "\n",
    "    # if we keep the list in memory, there may be problems with accessing the same index twice\n",
    "    indices = list()\n",
    "\n",
    "    if analyse_performance:\n",
    "        # let's analyse index performance\n",
    "        test_index_performance(collection_path, index_path)\n",
    "        # let's analyse search performance\n",
    "        for query in queries:\n",
    "            test_search_performance(collection_path, index_path, query, 5)\n",
    "\n",
    "def test_build(builder, collection):\n",
    "    stamp = time.time()\n",
    "    print(\"Building index with\", type(builder))\n",
    "    print(\"Collection:\", collection)\n",
    "    # this function should index the recieved collection and add it to the index\n",
    "    builder.build(collection)\n",
    "    # when we commit, the information in the index becomes persistent\n",
    "    # we can also save any extra information we may need\n",
    "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
    "    builder.commit()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def test_read(index, word):\n",
    "    stamp = time.time()\n",
    "    print(\"Reading index with\", type(index))\n",
    "    print(\"Collection size:\", index.ndocs())\n",
    "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
    "    # more tests\n",
    "    doc_id = 0\n",
    "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
    "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
    "    print(\"  Docs containing the word '\" + word + \"':\", index.doc_freq(word))\n",
    "    print(\"    First two documents:\", [(doc, freq) for doc, freq in index.postings(word)][0:2])\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def test_search (engine, index, query, cutoff):\n",
    "    stamp = time.time()\n",
    "    print(\"  \" + engine.__class__.__name__ + \" with index \" + index.__class__.__name__ + \" for query '\" + query + \"'\")\n",
    "    for path, score in engine.search(query, cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def disk_space(index_path: str) -> int:\n",
    "    space = 0\n",
    "    for f in os.listdir(index_path):\n",
    "        p = os.path.join(index_path, f)\n",
    "        if os.path.isfile(p):\n",
    "            space += os.path.getsize(p)\n",
    "    return space\n",
    "\n",
    "def test_index_performance (collection_path: str, base_index_path: str):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing index performance on \" + collection_path + \" document collection\")\n",
    "\n",
    "    print(\"  Build time...\")\n",
    "    start_time = time.time()\n",
    "    b = WhooshBuilder(base_index_path + \"whoosh\")\n",
    "    b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshForwardBuilder(base_index_path + \"whoosh_fwd\")\n",
    "    b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshPositionalBuilder(base_index_path + \"whoosh_pos\")\n",
    "    b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = RAMIndexBuilder(base_index_path + \"ram\")\n",
    "    b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    # start_time = time.time()\n",
    "    # b = DiskIndexBuilder(base_index_path + \"disk\")\n",
    "    # b.build(collection_path)\n",
    "    # b.commit()\n",
    "    # print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    print(\"  Load time...\")\n",
    "    start_time = time.time()\n",
    "    WhooshIndex(base_index_path + \"whoosh\")\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshForwardIndex(base_index_path + \"whoosh_fwd\")\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshPositionalIndex(base_index_path + \"whoosh_pos\")\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    RAMIndex(base_index_path + \"ram\")\n",
    "    print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    # start_time = time.time()\n",
    "    # DiskIndex(base_index_path + \"disk\")\n",
    "    # print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    print(\"  Disk space...\")\n",
    "    print(\"\\tWhooshIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh\")))\n",
    "    print(\"\\tWhooshForwardIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_fwd\")))\n",
    "    print(\"\\tWhooshPositionalIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_pos\")))\n",
    "    print(\"\\tRAMIndex: %s space ---\" % (disk_space(base_index_path + \"ram\")))\n",
    "    # print(\"\\tDiskIndex: %s space ---\" % (disk_space(base_index_path + \"disk\")))\n",
    "\n",
    "\n",
    "def test_search_performance (collection_name: str, base_index_path: str, query: str, cutoff: int):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing search performance on \" + collection_name + \" document collection with query: '\" + query + \"'\")\n",
    "    whoosh_index = WhooshIndex(base_index_path + \"whoosh\")\n",
    "    ram_index = RAMIndex(base_index_path + \"ram\")\n",
    "    # disk_index = DiskIndex(base_index_path + \"disk\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_search(WhooshSearcher(base_index_path + \"whoosh\"), whoosh_index, query, cutoff)\n",
    "    print(\"--- Whoosh on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    test_search(SlowVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "    print(\"--- SlowVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # let's test some combinations of ranking + index implementations\n",
    "    start_time = time.time()\n",
    "    test_search(TermBasedVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "    print(\"--- TermVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    test_search(TermBasedVSMSearcher(ram_index), ram_index, query, cutoff)\n",
    "    print(\"--- TermVSM on RAM %s seconds ---\" % (time.time() - start_time))\n",
    "    #start_time = time.time()\n",
    "    #test_search(TermBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "    #print(\"--- TermVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    #start_time = time.time()\n",
    "    #test_search(DocBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "    #print(\"--- DocVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "def test_pagerank(graphs_root_dir, cutoff):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing PageRank\")\n",
    "    # we separate this function because it cannot work with all the collections\n",
    "    start_time = time.time()\n",
    "    for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph1.dat\", 0.5, 50).rank(cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"--- Pagerank with toy_graph_1 %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph2.dat\", 0.6, 50).rank(cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"--- Pagerank with toy_graph_2 %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for path, score in PagerankDocScorer(graphs_root_dir + \"1k-links.dat\", 0.2, 50).rank(cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"--- Pagerank with simulated links for doc1k %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5JhJJSFiSl5"
   },
   "source": [
    "### Resumen de coste y rendimiento\n",
    "\n",
    "Hay que analizar las **diferencias de rendimiento** observadas entre las diferentes implementaciones que se han creado y probado para cada componente.\n",
    "\n",
    "En concreto, hay que reportar tiempo de indexado, consumo máximo de RAM y espacio en disco al construir el índice, y el tiempo de carga y consumo máximo de RAM al cargar el índice para cada una de las colecciones utilizadas.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "|      | Construcción | del | índice | Carga del | índice |\n",
    "|------|--------------------|-----------------|------------------|-----------------|-----------------|\n",
    "|      | Tiempo de indexado | Consumo máx RAM | Espacio en disco | Tiempo de carga | Consumo máx RAM |\n",
    "| toy1 | | | | | |\n",
    "| toy2 | | | | | |\n",
    "| 1K | | | | | |\n",
    "| 10K | | | | | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Enunciado P2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
